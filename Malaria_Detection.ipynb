{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Dense,Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 64\n",
    "dataset = []\n",
    "label = []\n",
    "image_directory = '/Users/kvno1ahmednagar/Desktop/DATASETS/cell_images/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parasitized_images = os.listdir('/Users/kvno1ahmednagar/Desktop/DATASETS/cell_images/Parasitized')\n",
    "for i,image_name in enumerate(parasitized_images):\n",
    "    \n",
    "    if(image_name.split('.')[1]=='png'):\n",
    "        image = cv2.imread('/Users/kvno1ahmednagar/Desktop/DATASETS/cell_images/Parasitized'+image_name)\n",
    "        #image = Image.fromarray(image,'RGB')\n",
    "        image = image.resize((SIZE,SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(1)\n",
    "\n",
    "\n",
    "        \n",
    "uninfected_images = os.listdir('/Users/kvno1ahmednagar/Desktop/DATASETS/cell_images/Uninfected')\n",
    "for i,image_name in enumerate(uninfected_images):\n",
    "    \n",
    "    if(image_name.split('.')[1]=='png'):\n",
    "        image = cv2.imread('/Users/kvno1ahmednagar/Desktop/DATASETS/cell_images/Uninfected'+image_name)\n",
    "        #image = Image.fromarray(image,'RGB')\n",
    "        image = image.resize((SIZE,SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(0)\n",
    "'''\n",
    "        \n",
    "        \n",
    "\n",
    "parasitized_images = os.listdir(image_directory + 'Parasitized/')\n",
    "for i, image_name in enumerate(parasitized_images):\n",
    "    \n",
    "    if (image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Parasitized/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE,SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(0)\n",
    "\n",
    "#Iterate through all images in Uninfected folder, resize to 64 x 64\n",
    "#Then save into the same numpy array 'dataset' but with label 1\n",
    "\n",
    "uninfected_images = os.listdir(image_directory + 'Uninfected/')\n",
    "for i, image_name in enumerate(uninfected_images):\n",
    "    if (image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Uninfected/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE,SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(dataset, to_categorical(np.array(label)), test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_23 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 30, 30, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 512)               3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 3,830,594\n",
      "Trainable params: 3,827,330\n",
      "Non-trainable params: 3,264\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nINPUT_SHAPE = (SIZE, SIZE, 3)   #change to (SIZE, SIZE, 3)\\ninp = keras.layers.Input(shape=INPUT_SHAPE)\\n\\nconv1 = keras.layers.Conv2D(32, kernel_size=(3, 3), \\n                               activation='relu', padding='same')(inp)\\npool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\\nnorm1 = keras.layers.BatchNormalization(axis = -1)(pool1)\\ndrop1 = keras.layers.Dropout(rate=0.2)(norm1)\\nconv2 = keras.layers.Conv2D(32, kernel_size=(3, 3), \\n                               activation='relu', padding='same')(drop1)\\npool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\\nnorm2 = keras.layers.BatchNormalization(axis = -1)(pool2)\\ndrop2 = keras.layers.Dropout(rate=0.2)(norm2)\\n\\nflat = keras.layers.Flatten()(drop2)  #Flatten the matrix to get it ready for dense.\\n\\nhidden1 = keras.layers.Dense(512, activation='relu')(flat)\\nnorm3 = keras.layers.BatchNormalization(axis = -1)(hidden1)\\ndrop3 = keras.layers.Dropout(rate=0.2)(norm3)\\nhidden2 = keras.layers.Dense(256, activation='relu')(drop3)\\nnorm4 = keras.layers.BatchNormalization(axis = -1)(hidden2)\\ndrop4 = keras.layers.Dropout(rate=0.2)(norm4)\\n\\nout = keras.layers.Dense(2, activation='sigmoid')(drop4)   #units=1 gives error\\n\\nmodel = keras.Model(inputs=inp, outputs=out)\\nmodel.compile(optimizer='adam',\\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\\n                metrics=['accuracy'])\\nprint(model.summary())\\n\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32,kernel_size = (3,3), input_shape = (SIZE,SIZE,3), activation = 'relu',padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 64,kernel_size = (3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (3,3)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(activation = 'relu',units = 512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(activation = 'relu',units = 1024))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(activation = 'sigmoid',units = 2))\n",
    "model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "INPUT_SHAPE = (SIZE, SIZE, 3)   #change to (SIZE, SIZE, 3)\n",
    "inp = keras.layers.Input(shape=INPUT_SHAPE)\n",
    "\n",
    "conv1 = keras.layers.Conv2D(32, kernel_size=(3, 3), \n",
    "                               activation='relu', padding='same')(inp)\n",
    "pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "norm1 = keras.layers.BatchNormalization(axis = -1)(pool1)\n",
    "drop1 = keras.layers.Dropout(rate=0.2)(norm1)\n",
    "conv2 = keras.layers.Conv2D(32, kernel_size=(3, 3), \n",
    "                               activation='relu', padding='same')(drop1)\n",
    "pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "norm2 = keras.layers.BatchNormalization(axis = -1)(pool2)\n",
    "drop2 = keras.layers.Dropout(rate=0.2)(norm2)\n",
    "\n",
    "flat = keras.layers.Flatten()(drop2)  #Flatten the matrix to get it ready for dense.\n",
    "\n",
    "hidden1 = keras.layers.Dense(512, activation='relu')(flat)\n",
    "norm3 = keras.layers.BatchNormalization(axis = -1)(hidden1)\n",
    "drop3 = keras.layers.Dropout(rate=0.2)(norm3)\n",
    "hidden2 = keras.layers.Dense(256, activation='relu')(drop3)\n",
    "norm4 = keras.layers.BatchNormalization(axis = -1)(hidden2)\n",
    "drop4 = keras.layers.Dropout(rate=0.2)(norm4)\n",
    "\n",
    "out = keras.layers.Dense(2, activation='sigmoid')(drop4)   #units=1 gives error\n",
    "\n",
    "model = keras.Model(inputs=inp, outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n",
    "                metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147456"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[31].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19841 samples, validate on 2205 samples\n",
      "Epoch 1/20\n",
      "19841/19841 [==============================] - 137s 7ms/step - loss: 0.4606 - accuracy: 0.7927 - val_loss: 1.9402 - val_accuracy: 0.7147\n",
      "Epoch 2/20\n",
      "19841/19841 [==============================] - 135s 7ms/step - loss: 0.2334 - accuracy: 0.9129 - val_loss: 0.5570 - val_accuracy: 0.8653\n",
      "Epoch 3/20\n",
      "19841/19841 [==============================] - 140s 7ms/step - loss: 0.1956 - accuracy: 0.9276 - val_loss: 0.7004 - val_accuracy: 0.8553\n",
      "Epoch 4/20\n",
      "19841/19841 [==============================] - 141s 7ms/step - loss: 0.1724 - accuracy: 0.9360 - val_loss: 0.6088 - val_accuracy: 0.8676\n",
      "Epoch 5/20\n",
      "19841/19841 [==============================] - 148s 7ms/step - loss: 0.1604 - accuracy: 0.9402 - val_loss: 0.2395 - val_accuracy: 0.9048\n",
      "Epoch 6/20\n",
      "19841/19841 [==============================] - 168s 8ms/step - loss: 0.1487 - accuracy: 0.9432 - val_loss: 0.4085 - val_accuracy: 0.8372\n",
      "Epoch 7/20\n",
      "19841/19841 [==============================] - 139s 7ms/step - loss: 0.1410 - accuracy: 0.9486 - val_loss: 0.2541 - val_accuracy: 0.9175\n",
      "Epoch 8/20\n",
      "19841/19841 [==============================] - 132s 7ms/step - loss: 0.1332 - accuracy: 0.9515 - val_loss: 0.4598 - val_accuracy: 0.8776\n",
      "Epoch 9/20\n",
      "19841/19841 [==============================] - 144s 7ms/step - loss: 0.1222 - accuracy: 0.9538 - val_loss: 0.2438 - val_accuracy: 0.9066\n",
      "Epoch 10/20\n",
      "19841/19841 [==============================] - 155s 8ms/step - loss: 0.1158 - accuracy: 0.9566 - val_loss: 0.1945 - val_accuracy: 0.9370\n",
      "Epoch 11/20\n",
      "19841/19841 [==============================] - 156s 8ms/step - loss: 0.1062 - accuracy: 0.9602 - val_loss: 0.1894 - val_accuracy: 0.9297\n",
      "Epoch 12/20\n",
      "19841/19841 [==============================] - 144s 7ms/step - loss: 0.0984 - accuracy: 0.9622 - val_loss: 0.1854 - val_accuracy: 0.9442\n",
      "Epoch 13/20\n",
      "19841/19841 [==============================] - 169s 9ms/step - loss: 0.0941 - accuracy: 0.9649 - val_loss: 0.2924 - val_accuracy: 0.9152\n",
      "Epoch 14/20\n",
      "19841/19841 [==============================] - 181s 9ms/step - loss: 0.0904 - accuracy: 0.9650 - val_loss: 0.1786 - val_accuracy: 0.9438\n",
      "Epoch 15/20\n",
      "19841/19841 [==============================] - 184s 9ms/step - loss: 0.0811 - accuracy: 0.9692 - val_loss: 0.1703 - val_accuracy: 0.9478\n",
      "Epoch 16/20\n",
      "19841/19841 [==============================] - 168s 8ms/step - loss: 0.0791 - accuracy: 0.9699 - val_loss: 0.2308 - val_accuracy: 0.9265\n",
      "Epoch 17/20\n",
      "19841/19841 [==============================] - 197s 10ms/step - loss: 0.0743 - accuracy: 0.9704 - val_loss: 0.2734 - val_accuracy: 0.9288\n",
      "Epoch 18/20\n",
      "19841/19841 [==============================] - 190s 10ms/step - loss: 0.0616 - accuracy: 0.9764 - val_loss: 0.2181 - val_accuracy: 0.9474\n",
      "Epoch 19/20\n",
      "19841/19841 [==============================] - 186s 9ms/step - loss: 0.0610 - accuracy: 0.9772 - val_loss: 0.1831 - val_accuracy: 0.9519\n",
      "Epoch 20/20\n",
      "19841/19841 [==============================] - 255s 13ms/step - loss: 0.0626 - accuracy: 0.9763 - val_loss: 0.1930 - val_accuracy: 0.9515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x65a404f50>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_train),Y_train,batch_size = 128,epochs = 20,validation_split = 0.1)\n",
    "\n",
    "#history = model.fit(np.array(X_train), y_train, batch_size = 64, verbose = 1, epochs = 25, validation_split = 0.1, shuffle = False #callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137790"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110232"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27558"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) + len(X_valid) == len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].size == X_train[17].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
